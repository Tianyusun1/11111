# File: stage2_generation/scripts/train_taiyi.py (V8.7: Single-Stream + Learnable Multi-Scale Weights)

import argparse
import logging
import os
import math
import random
from pathlib import Path
import sys
import matplotlib.pyplot as plt

# =========================================================
# [CRITICAL PATCH] ä¿®å¤å—é™ç¯å¢ƒä¸‹çš„ PermissionError
# =========================================================
try:
    EnvironClass = os.environ.__class__
    _orig_setitem = EnvironClass.__setitem__
    _orig_delitem = EnvironClass.__delitem__

    def _safe_setitem(self, key, value):
        try:
            _orig_setitem(self, key, value)
        except PermissionError:
            pass
        except Exception as e:
            raise e

    def _safe_delitem(self, key):
        try:
            _orig_delitem(self, key)
        except PermissionError:
            pass
        except KeyError:
            pass
        except Exception as e:
            raise e

    EnvironClass.__setitem__ = _safe_setitem
    EnvironClass.__delitem__ = _safe_delitem
    
    def _safe_clear(self):
        keys = list(self.keys())
        for key in keys:
            self.pop(key, None)
            
    EnvironClass.clear = _safe_clear
    print("âœ… Environment monkey-patch applied successfully.")
except Exception as e:
    print(f"âš ï¸ Failed to patch environment: {e}")

import torch
import torch.nn.functional as F
import transformers
from accelerate import Accelerator
from accelerate.logging import get_logger
from datasets import load_dataset
from PIL import Image
from torchvision import transforms
from tqdm.auto import tqdm

import diffusers
from diffusers import (
    AutoencoderKL,
    ControlNetModel,
    DDPMScheduler,
    UNet2DConditionModel,
    StableDiffusionControlNetPipeline,
)
from peft import LoraConfig, get_peft_model

logger = get_logger(__name__)

# =========================================================
# [NEW] æ¶æ„åˆ›æ–°ï¼šå¯å­¦ä¹ çš„å¤šå°ºåº¦æƒé‡æ¨¡å—
# =========================================================
class ControlNetScaler(torch.nn.Module):
    """
    è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—ï¼š
    ä¸º ControlNet è¾“å‡ºçš„ 13 ä¸ªç‰¹å¾å±‚ (12 DownBlocks + 1 MidBlock) 
    åˆ†åˆ«å­¦ä¹ ä¸€ä¸ªç‹¬ç«‹çš„æƒé‡ç³»æ•°ã€‚
    """
    def __init__(self, num_scales=13, init_value=1.0):
        super().__init__()
        # åˆå§‹åŒ–ä¸º 1.0ï¼Œè¡¨ç¤ºä»æ ‡å‡† ControlNet çŠ¶æ€å¼€å§‹å¾®è°ƒ
        self.scales = torch.nn.Parameter(torch.full((num_scales,), init_value, dtype=torch.float32))

    def forward(self, down_samples, mid_sample):
        # down_samples: list of 12 tensors
        # mid_sample: 1 tensor
        
        weighted_down = []
        for i, sample in enumerate(down_samples):
            # å°†å¯¹åº”å±‚çš„æƒé‡å¹¿æ’­å¹¶ç›¸ä¹˜
            weighted_down.append(sample * self.scales[i])
            
        weighted_mid = mid_sample * self.scales[-1]
        
        return weighted_down, weighted_mid

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pretrained_model_name_or_path", type=str, default="Idea-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1")
    parser.add_argument("--output_dir", type=str, default="taiyi_controlnet_lora_output")
    parser.add_argument("--train_data_dir", type=str, required=True)
    parser.add_argument("--resolution", type=int, default=512)
    parser.add_argument("--train_batch_size", type=int, default=4) 
    parser.add_argument("--num_train_epochs", type=int, default=10)
    
    # [CONFIG] å­¦ä¹ ç‡è®¾ç½®
    parser.add_argument("--learning_rate", type=float, default=1e-5, help="ControlNetçš„å­¦ä¹ ç‡")
    parser.add_argument("--learning_rate_lora", type=float, default=1e-4, help="UNet LoRAçš„å­¦ä¹ ç‡")
    
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--mixed_precision", type=str, default="fp16") 
    parser.add_argument("--checkpointing_steps", type=int, default=2000)
    parser.add_argument("--lambda_struct", type=float, default=0.1, help="[å·²åºŸå¼ƒ] ç»“æ„å¯¹é½æŸå¤±æƒé‡")
    parser.add_argument("--lora_rank", type=int, default=32, help="LoRAçš„ç§©")
    
    # [CONFIG] V8.6 æ™ºèƒ½å†»ç»“
    parser.add_argument("--smart_freeze", action="store_true", default=True, help="å¼€å¯æ™ºèƒ½å†»ç»“ï¼šåªè®­ç»ƒè¾“å…¥/è¾“å‡ºå±‚")
    
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    accelerator = Accelerator(
        mixed_precision=args.mixed_precision,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
    )
    device = accelerator.device

    if accelerator.is_main_process:
        logging.basicConfig(
            format="%(asctime)s - %(levelname)s - %(message)s",
            datefmt="%m/%d/%Y %H:%M:%S",
            level=logging.INFO,
        )
        log_file = os.path.join(args.output_dir, "train_loss_history.txt")
        file_handler = logging.FileHandler(log_file, mode='a')
        file_handler.setFormatter(logging.Formatter("%(asctime)s - %(message)s"))
        logger.logger.addHandler(file_handler)
        logger.info(f"âœ¨ [V8.7 å•æµ+è‡ªé€‚åº”æƒé‡ç‰ˆ] å¯åŠ¨ï¼")
        logger.info(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")

    # 1. åŠ è½½æ¨¡å‹
    tokenizer = transformers.BertTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer")
    text_encoder = transformers.BertModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="unet")

    # åˆå§‹åŒ–å•æµ ControlNet
    if accelerator.is_main_process:
        print("æ­£åœ¨åˆå§‹åŒ–å•æµ ControlNet (Structure Stream)...")
    controlnet = ControlNetModel.from_unet(unet)

    # 2. LoRA è®¾ç½® (è´Ÿè´£é£æ ¼)
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    unet.requires_grad_(False) 
    
    unet_lora_config = LoraConfig(
        r=args.lora_rank,
        lora_alpha=args.lora_rank,
        init_lora_weights="gaussian",
        target_modules=["to_k", "to_q", "to_v", "to_out.0", "add_k_proj", "add_v_proj"],
    )
    unet = get_peft_model(unet, unet_lora_config)
    
    # [NEW] åˆå§‹åŒ–å¯å­¦ä¹ æƒé‡æ¨¡å— (13å±‚)
    control_scaler = ControlNetScaler(num_scales=13, init_value=1.0)
    control_scaler.to(device)
    control_scaler.train()

    if accelerator.is_main_process:
        print("âœ… LoRA æ³¨å…¥æˆåŠŸ (è´Ÿè´£æ°´å¢¨é£æ ¼å­¦ä¹ )")
        print("âœ… è‡ªé€‚åº”å¤šå°ºåº¦æƒé‡æ¨¡å— (ControlNetScaler) å·²åˆå§‹åŒ–")
        unet.print_trainable_parameters()

    # æ˜¾å­˜ä¼˜åŒ–
    try:
        unet.enable_xformers_memory_efficient_attention()
        controlnet.enable_xformers_memory_efficient_attention()
        controlnet.enable_gradient_checkpointing()
        unet.enable_gradient_checkpointing()
    except Exception:
        pass

    # =========================================================
    # [Smart Freeze] æ™ºèƒ½å†»ç»“é€»è¾‘
    # =========================================================
    if args.smart_freeze:
        controlnet.requires_grad_(False) # å…ˆå…¨å†»ç»“
        trainable_names = []
        
        # 1. è§£å†»è¾“å…¥å±‚ (Hint Block)
        for n, p in controlnet.controlnet_cond_embedding.named_parameters():
            p.requires_grad = True
            trainable_names.append(n)
        for n, p in controlnet.conv_in.named_parameters():
            p.requires_grad = True
            trainable_names.append(n)
            
        # 2. è§£å†»è¾“å‡ºå±‚ (Zero Convolutions)
        for n, p in controlnet.controlnet_down_blocks.named_parameters():
            p.requires_grad = True
            trainable_names.append(n)
        for n, p in controlnet.controlnet_mid_block.named_parameters():
            p.requires_grad = True
            trainable_names.append(n)
            
        if accelerator.is_main_process:
            print(f"â„ï¸ [Smart Freeze] æ™ºèƒ½å†»ç»“å·²åº”ç”¨ï¼ä»…è®­ç»ƒ Adapter å±‚å’Œ Zero Convolutionã€‚")

    # 3. ä¼˜åŒ–å™¨ (åŠ å…¥ control_scaler)
    params_to_optimize = [
        {"params": filter(lambda p: p.requires_grad, controlnet.parameters()), "lr": args.learning_rate},
        {"params": unet.parameters(), "lr": args.learning_rate_lora},
        # [NEW] æƒé‡æ¨¡å—çš„å­¦ä¹ ç‡ (ç¨å¾®ç»™å¤§ä¸€ç‚¹ï¼Œè®©å®ƒèƒ½åŠ¨èµ·æ¥)
        {"params": control_scaler.parameters(), "lr": 1e-3} 
    ]
    optimizer = torch.optim.AdamW(params_to_optimize)

    # 4. æ•°æ®åŠ è½½
    raw_dataset = load_dataset("json", data_files=os.path.join(args.train_data_dir, "train.jsonl"))["train"]
    train_testvalid = raw_dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = train_testvalid['train']
    val_dataset = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)['train']

    transform = transforms.Compose([
        transforms.Resize((args.resolution, args.resolution)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])
    cond_transform = transforms.Compose([
        transforms.Resize((args.resolution, args.resolution)),
        transforms.ToTensor(), 
    ])

    def collate_fn(examples):
        pixel_values, cond_pixel_values, input_ids, raw_texts = [], [], [], []
        for example in examples:
            try:
                img_path = os.path.join(args.train_data_dir, example["image"])
                cond_path = os.path.join(args.train_data_dir, example["conditioning_image"])
                pixel_values.append(transform(Image.open(img_path).convert("RGB")))
                cond_pixel_values.append(cond_transform(Image.open(cond_path).convert("RGB")))
                caption = example["text"]
                inputs = tokenizer(caption, max_length=tokenizer.model_max_length, 
                                 padding="max_length", truncation=True, return_tensors="pt")
                input_ids.append(inputs.input_ids[0])
                raw_texts.append(example["text"])
            except: continue
        return {
            "pixel_values": torch.stack(pixel_values),
            "conditioning_pixel_values": torch.stack(cond_pixel_values),
            "input_ids": torch.stack(input_ids),
            "texts": raw_texts
        }

    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4
    )
    val_dataloader = torch.utils.data.DataLoader(
        val_dataset, batch_size=args.train_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4
    )

    # [IMPORTANT] åŠ å…¥ control_scaler åˆ° prepare
    controlnet, unet, control_scaler, optimizer, train_dataloader, val_dataloader = accelerator.prepare(
        controlnet, unet, control_scaler, optimizer, train_dataloader, val_dataloader
    )
    
    vae.to(device, dtype=torch.float16)
    text_encoder.to(device, dtype=torch.float16)

    # Loss è®°å½• (ç§»é™¤äº† struct loss)
    loss_history = {'steps': [], 'total': [], 'mse': []}

    def plot_loss_curve(history, save_path):
        if len(history['steps']) < 2: return
        plt.figure(figsize=(10, 6))
        plt.plot(history['steps'], history['total'], label='Total Loss', color='blue', alpha=0.6, linewidth=1)
        plt.plot(history['steps'], history['mse'], label='MSE Loss', color='orange', alpha=0.5, linestyle='--', linewidth=1)
        plt.title(f"Training Loss (Step {history['steps'][-1]})")
        plt.xlabel("Steps")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        try:
            plt.savefig(save_path)
            plt.close()
        except: pass

    # 5. è®­ç»ƒå¾ªç¯
    global_step = 0
    if accelerator.is_main_process:
        print(f"ğŸš€ å¯åŠ¨è®­ç»ƒæµç¨‹...")
        
    for epoch in range(args.num_train_epochs):
        controlnet.train()
        unet.train()
        control_scaler.train()
        
        train_loss_epoch = 0.0
        
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(controlnet, unet, control_scaler):
                target_images = batch["pixel_values"].to(dtype=torch.float16)
                latents = vae.encode(target_images).latent_dist.sample() * vae.config.scaling_factor
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device).long()
                scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
                noisy_latents = scheduler.add_noise(latents, noise, timesteps)
                
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]
                cond_image = batch["conditioning_pixel_values"].to(dtype=torch.float16)
                
                # Condition Dropout (ä¿è¯æ§åˆ¶åŠ›çš„å…³é”®)
                rand_dropout = random.random()
                if rand_dropout < 0.15:
                    cond_input = torch.zeros_like(cond_image) # ç©º Mask
                else:
                    cond_input = cond_image # æ­£å¸¸çº¹ç† Mask
                
                # ControlNet å‰å‘
                down_block_res_samples, mid_block_res_sample = controlnet(
                    noisy_latents, 
                    timesteps, 
                    encoder_hidden_states, 
                    cond_input, 
                    return_dict=False
                )

                # [NEW] åº”ç”¨å¯å­¦ä¹ çš„å±‚çº§æƒé‡
                # æ³¨æ„ï¼šæ˜¾å¼è½¬æ¢ä¸º fp16 ä»¥åŒ¹é… UNet è¾“å…¥
                weighted_down, weighted_mid = control_scaler(
                    down_block_res_samples, 
                    mid_block_res_sample
                )
                
                weighted_down = [s.to(dtype=torch.float16) for s in weighted_down]
                weighted_mid = weighted_mid.to(dtype=torch.float16)

                # UNet å‰å‘
                model_pred = unet(
                    noisy_latents, 
                    timesteps, 
                    encoder_hidden_states, 
                    down_block_additional_residuals=weighted_down,
                    mid_block_additional_residual=weighted_mid
                ).sample

                # [CLEAN] ä»…ä½¿ç”¨æ ‡å‡† MSE Loss
                loss_ddpm = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")
                total_loss = loss_ddpm 
                
                accelerator.backward(total_loss)
                optimizer.step()
                optimizer.zero_grad()
            
            train_loss_epoch += total_loss.item()
            global_step += 1
            
            # Checkpoint ä¿å­˜
            if global_step % args.checkpointing_steps == 0 and accelerator.is_main_process:
                ckpt_dir = Path(args.output_dir) / f"checkpoint-{global_step}"
                os.makedirs(ckpt_dir, exist_ok=True)
                accelerator.unwrap_model(controlnet).save_pretrained(ckpt_dir / "controlnet_structure") 
                accelerator.unwrap_model(unet).save_pretrained(ckpt_dir / "unet_lora")
                # [NEW] ä¿å­˜ Scale æƒé‡
                torch.save(accelerator.unwrap_model(control_scaler).state_dict(), ckpt_dir / "scaler.pth")
                print(f"ğŸ’¾ Checkpoint saved at step {global_step}")

            # æ—¥å¿—ä¸ç»˜å›¾
            if step % 10 == 0 and accelerator.is_main_process:
                lr_c = optimizer.param_groups[0]['lr']
                
                loss_history['steps'].append(global_step)
                loss_history['total'].append(total_loss.item())
                loss_history['mse'].append(loss_ddpm.item())
                
                # [NEW] æ‰“å°æƒé‡åˆ†å¸ƒç®€æŠ¥
                current_scales = accelerator.unwrap_model(control_scaler).scales.detach().cpu().numpy()
                scale_str = ", ".join([f"{s:.2f}" for s in current_scales])
                low_avg = current_scales[:4].mean()
                high_avg = current_scales[8:].mean()

                msg = (f"Ep {epoch+1} | Step {step} | Loss: {total_loss.item():.4f} | LR: {lr_c:.1e}")
                print(msg)
                print(f"   âš–ï¸ Scales: [{scale_str}] (Low: {low_avg:.2f} / High: {high_avg:.2f})")
                logger.info(msg)
                
                # è®°å½• Scale å†å²
                with open(os.path.join(args.output_dir, "scales_history.csv"), "a") as f:
                    f.write(f"{global_step}," + ",".join(map(str, current_scales)) + "\n")
                
                if step % 100 == 0:
                    plot_loss_curve(loss_history, os.path.join(args.output_dir, "loss_curve.png"))

        # === éªŒè¯ ===
        if accelerator.is_main_process:
            print(f"ğŸ” Epoch {epoch}: éªŒè¯ä¸­...")
            plot_loss_curve(loss_history, os.path.join(args.output_dir, "loss_curve.png"))
        
        controlnet.eval()
        unet.eval()
        
        try:
            if accelerator.is_main_process:
                with torch.autocast(device.type, dtype=torch.float16):
                    with torch.no_grad():
                        unwrapped_net = accelerator.unwrap_model(controlnet)
                        unwrapped_unet = accelerator.unwrap_model(unet)
                        val_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
                        
                        # æ ‡å‡†éªŒè¯ Pipeline (æ³¨ï¼šè¿™é‡Œåªèƒ½éªŒè¯é»˜è®¤ Scale=1.0 çš„æ•ˆæœï¼Œ
                        # å› ä¸ºæ ‡å‡† Pipeline ä¸æ”¯æŒä¼ å…¥è‡ªå®šä¹‰ Layer Weightsã€‚
                        # ä½†æ ¸å¿ƒç›®çš„æ˜¯æ£€æŸ¥æ¨¡å‹æœ‰æ²¡æœ‰å´©ï¼Œè¿™å°±å¤Ÿäº†ã€‚)
                        pipe = StableDiffusionControlNetPipeline(
                            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer,
                            unet=unwrapped_unet, 
                            controlnet=unwrapped_net, 
                            scheduler=val_scheduler, safety_checker=None, feature_extractor=None
                        ).to(device)
                        
                        test_batch = next(iter(val_dataloader))
                        test_cond = test_batch["conditioning_pixel_values"][0:1].to(device=device, dtype=torch.float16)
                        
                        layout_img_pil = transforms.ToPILImage()(test_cond.squeeze(0).cpu())
                        layout_img_pil.save(Path(args.output_dir) / f"layout_epoch_{epoch}_val.png")

                        sample_out = pipe(
                            prompt="å±±ç©·æ°´å¤ç–‘æ— è·¯ï¼ŒæŸ³æš—èŠ±æ˜åˆä¸€æ‘ã€‚", 
                            image=test_cond, 
                            num_inference_steps=20,
                            guidance_scale=7.5
                        ).images[0]
                        sample_out.save(Path(args.output_dir) / f"sample_epoch_{epoch}_val.png")
                        print(f"âœ… éªŒè¯å›¾å·²ä¿å­˜")
                        del pipe
                        torch.cuda.empty_cache()
        except Exception as e:
            print(f"éªŒè¯é‡‡æ ·å¤±è´¥: {e}")

    if accelerator.is_main_process:
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        save_path_c = Path(args.output_dir) / "controlnet_structure"
        os.makedirs(save_path_c, exist_ok=True)
        accelerator.unwrap_model(controlnet).save_pretrained(save_path_c)
        accelerator.unwrap_model(unet).save_pretrained(Path(args.output_dir) / "unet_lora")
        # [NEW] ä¿å­˜æœ€ç»ˆæƒé‡
        torch.save(accelerator.unwrap_model(control_scaler).state_dict(), Path(args.output_dir) / "scaler_final.pth")
        
        plot_loss_curve(loss_history, os.path.join(args.output_dir, "loss_curve_final.png"))
        print(f"âœ… è®­ç»ƒå®Œæˆï¼ŒLoss æ›²çº¿å·²ä¿å­˜ã€‚")

if __name__ == "__main__":
    main()