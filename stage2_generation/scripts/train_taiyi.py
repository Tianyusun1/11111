import argparse
import logging
import os
import math
import random
from pathlib import Path

# =========================================================
# [CRITICAL PATCH] ä¿®å¤å—é™ç¯å¢ƒä¸‹çš„ PermissionError
# =========================================================
try:
    EnvironClass = os.environ.__class__
    _orig_setitem = EnvironClass.__setitem__
    _orig_delitem = EnvironClass.__delitem__

    def _safe_setitem(self, key, value):
        try:
            _orig_setitem(self, key, value)
        except PermissionError:
            pass
        except Exception as e:
            raise e

    def _safe_delitem(self, key):
        try:
            _orig_delitem(self, key)
        except PermissionError:
            pass
        except KeyError:
            pass
        except Exception as e:
            raise e

    EnvironClass.__setitem__ = _safe_setitem
    EnvironClass.__delitem__ = _safe_delitem
    
    def _safe_clear(self):
        keys = list(self.keys())
        for key in keys:
            self.pop(key, None)
            
    EnvironClass.clear = _safe_clear
    print("âœ… Environment monkey-patch applied successfully.")
except Exception as e:
    print(f"âš ï¸ Failed to patch environment: {e}")

import torch
import torch.nn.functional as F
import transformers
from accelerate import Accelerator
from accelerate.logging import get_logger
from datasets import load_dataset
from PIL import Image
from torchvision import transforms
from tqdm.auto import tqdm

import diffusers
from diffusers import (
    AutoencoderKL,
    ControlNetModel,
    DDPMScheduler,
    UNet2DConditionModel,
    StableDiffusionControlNetPipeline,
)
from diffusers.optimization import get_scheduler

# [NEW] å¼•å…¥ LoRA åº“ï¼Œç”¨äºå®‰å…¨å¾®è°ƒåº•åº§
from peft import LoraConfig, get_peft_model

logger = get_logger(__name__)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--pretrained_model_name_or_path", type=str, default="Idea-CCNL/Taiyi-Stable-Diffusion-1B-Chinese-v0.1")
    parser.add_argument("--output_dir", type=str, default="taiyi_controlnet_lora_output")
    parser.add_argument("--train_data_dir", type=str, required=True)
    parser.add_argument("--resolution", type=int, default=512)
    parser.add_argument("--train_batch_size", type=int, default=4) 
    parser.add_argument("--num_train_epochs", type=int, default=10)
    
    # [CONFIG] å­¦ä¹ ç‡è®¾ç½®
    parser.add_argument("--learning_rate", type=float, default=1e-5, help="ControlNetçš„å­¦ä¹ ç‡")
    parser.add_argument("--learning_rate_lora", type=float, default=1e-4, help="UNet LoRAçš„å­¦ä¹ ç‡")
    
    parser.add_argument("--gradient_accumulation_steps", type=int, default=1)
    parser.add_argument("--mixed_precision", type=str, default="fp16") 
    parser.add_argument("--checkpointing_steps", type=int, default=2000)
    parser.add_argument("--lambda_struct", type=float, default=0.1, help="ç»“æ„å¯¹é½æŸå¤±æƒé‡")
    
    # [CONFIG] LoRA è®¾ç½®
    parser.add_argument("--lora_rank", type=int, default=32, help="LoRAçš„ç§©ï¼Œè¶Šå¤§é€‚åº”èƒ½åŠ›è¶Šå¼ºä½†å‚æ•°è¶Šå¤š")
    
    args = parser.parse_args()

    # [FIX] å¯åŠ¨å‰ç¡®ä¿è¾“å‡ºä¸»ç›®å½•å­˜åœ¨
    os.makedirs(args.output_dir, exist_ok=True)

    accelerator = Accelerator(
        mixed_precision=args.mixed_precision,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
    )
    device = accelerator.device

    # 1. åŠ è½½åŸºç¡€æ¨¡å‹ç»„ä»¶
    tokenizer = transformers.BertTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer")
    text_encoder = transformers.BertModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="text_encoder")
    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder="vae")
    unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="unet")

    if accelerator.is_main_process:
        print("æ­£åœ¨ä» UNet åˆå§‹åŒ– ç»“æ„-é£æ ¼ åŒæµ ControlNet...")
    controlnet_s = ControlNetModel.from_unet(unet)
    controlnet_t = ControlNetModel.from_unet(unet)

    # 2. å†»ç»“ä¸ LoRA æ³¨å…¥ç­–ç•¥
    # é¦–å…ˆå†»ç»“æ‰€æœ‰æ¨¡å‹
    vae.requires_grad_(False)
    text_encoder.requires_grad_(False)
    unet.requires_grad_(False) # UNet ä¸»å¹²å†»ç»“
    
    # [NEW] é…ç½® UNet çš„ LoRA
    # é’ˆå¯¹ Attention æ¨¡å—æ³¨å…¥é€‚é…å™¨ï¼Œä½¿å…¶å­¦ä¹ æ°´å¢¨ç”»é£
    unet_lora_config = LoraConfig(
        r=args.lora_rank,
        lora_alpha=args.lora_rank,
        init_lora_weights="gaussian",
        target_modules=["to_k", "to_q", "to_v", "to_out.0", "add_k_proj", "add_v_proj"],
    )
    
    # å°† LoRA æŒ‚è½½åˆ° UNet (unet å˜ä¸º PeftModel)
    unet = get_peft_model(unet, unet_lora_config)
    
    if accelerator.is_main_process:
        print("âœ… LoRA æ³¨å…¥æˆåŠŸï¼ŒUNet å¯è®­ç»ƒå‚æ•°å¦‚ä¸‹:")
        unet.print_trainable_parameters()

    # å¼€å¯ xformers æ˜¾å­˜ä¼˜åŒ–
    try:
        # LoRA è®­ç»ƒæ—¶è‹¥ä½¿ç”¨ fp16 å¯èƒ½ä¼šæœ‰æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼Œä½† xformers é€šå¸¸èƒ½å¤„ç†
        unet.enable_xformers_memory_efficient_attention()
        controlnet_s.enable_xformers_memory_efficient_attention()
        controlnet_t.enable_xformers_memory_efficient_attention()
    except Exception as e:
        if accelerator.is_main_process:
            print(f"Warning: xformers æœªå®‰è£…æˆ–ä¸å¯ç”¨: {e}")

    # 3. ä¼˜åŒ–å™¨å‡†å¤‡ (åˆ†ç»„å­¦ä¹ ç‡)
    params_to_optimize = [
        {"params": controlnet_s.parameters(), "lr": args.learning_rate},
        {"params": controlnet_t.parameters(), "lr": args.learning_rate},
        {"params": unet.parameters(), "lr": args.learning_rate_lora} # è¿™é‡Œå®é™…ä¼˜åŒ–çš„æ˜¯ LoRA å‚æ•°
    ]
    optimizer = torch.optim.AdamW(params_to_optimize)

    # 4. æ•°æ®åŠ è½½ä¸åˆ’åˆ†
    raw_dataset = load_dataset("json", data_files=os.path.join(args.train_data_dir, "train.jsonl"))["train"]
    
    train_testvalid = raw_dataset.train_test_split(test_size=0.1, seed=42)
    train_dataset = train_testvalid['train']
    
    test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)
    val_dataset = test_valid['train']
    test_dataset = test_valid['test']
    
    if accelerator.is_main_process:
        print(f"ğŸ“Š æ•°æ®é›†åˆ’åˆ†å®Œæˆ: Train={len(train_dataset)} | Val={len(val_dataset)} | Test={len(test_dataset)}")

    transform = transforms.Compose([
        transforms.Resize((args.resolution, args.resolution)),
        transforms.ToTensor(),
        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
    ])
    cond_transform = transforms.Compose([
        transforms.Resize((args.resolution, args.resolution)),
        transforms.ToTensor(), 
    ])

    def collate_fn(examples):
        pixel_values, cond_pixel_values, input_ids, raw_texts = [], [], [], []
        for example in examples:
            try:
                img_path = os.path.join(args.train_data_dir, example["image"])
                cond_path = os.path.join(args.train_data_dir, example["conditioning_image"])
                pixel_values.append(transform(Image.open(img_path).convert("RGB")))
                cond_pixel_values.append(cond_transform(Image.open(cond_path).convert("RGB")))
                
                # [USER] å§‹ç»ˆä½¿ç”¨å®Œæ•´æ–‡æœ¬
                caption = example["text"]
                inputs = tokenizer(caption, max_length=tokenizer.model_max_length, 
                                 padding="max_length", truncation=True, return_tensors="pt")
                input_ids.append(inputs.input_ids[0])
                raw_texts.append(example["text"])
            except: continue
        return {
            "pixel_values": torch.stack(pixel_values),
            "conditioning_pixel_values": torch.stack(cond_pixel_values),
            "input_ids": torch.stack(input_ids),
            "texts": raw_texts
        }

    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, batch_size=args.train_batch_size, shuffle=True, collate_fn=collate_fn, num_workers=4
    )
    val_dataloader = torch.utils.data.DataLoader(
        val_dataset, batch_size=args.train_batch_size, shuffle=False, collate_fn=collate_fn, num_workers=4
    )

    # å‡†å¤‡ Accelerator (æ³¨æ„ UNet ä¹Ÿè¦ prepare)
    controlnet_s, controlnet_t, unet, optimizer, train_dataloader, val_dataloader = accelerator.prepare(
        controlnet_s, controlnet_t, unet, optimizer, train_dataloader, val_dataloader
    )
    
    vae.to(device, dtype=torch.float16)
    text_encoder.to(device, dtype=torch.float16)

    # 5. è®­ç»ƒå¾ªç¯
    global_step = 0
    if accelerator.is_main_process:
        print(f"ğŸš€ å¯åŠ¨å®‰å…¨è®­ç»ƒ (LoRA + ControlNet Dropout)")
        
    for epoch in range(args.num_train_epochs):
        controlnet_s.train()
        controlnet_t.train()
        unet.train()
        
        train_loss_epoch = 0.0
        
        for step, batch in enumerate(train_dataloader):
            with accelerator.accumulate(controlnet_s, controlnet_t, unet):
                target_images = batch["pixel_values"].to(dtype=torch.float16)
                latents = vae.encode(target_images).latent_dist.sample() * vae.config.scaling_factor
                
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device).long()
                scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
                noisy_latents = scheduler.add_noise(latents, noise, timesteps)
                
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]
                cond_image = batch["conditioning_pixel_values"].to(dtype=torch.float16)
                
                # [STRATEGY] ControlNet Dropout
                # éšæœºå±è”½æŸä¸€æµï¼Œè¿«ä½¿æ¨¡å‹ç‹¬ç«‹å­¦ä¹ æ¯ä¸€æµçš„ç‰¹å¾
                # 85% åŒæµ | 7.5% ä»…ç»“æ„ | 7.5% ä»…é£æ ¼
                rand_dropout = random.random()
                cond_s = cond_image
                cond_t = cond_image
                
                if rand_dropout < 0.075:
                    cond_s = torch.zeros_like(cond_image) # å±è”½ Structure
                elif rand_dropout < 0.15:
                    cond_t = torch.zeros_like(cond_image) # å±è”½ Style
                
                down_s, mid_s = controlnet_s(noisy_latents, timesteps, encoder_hidden_states, cond_s, return_dict=False)
                down_t, mid_t = controlnet_t(noisy_latents, timesteps, encoder_hidden_states, cond_t, return_dict=False)
                
                down_res = [(s.to(dtype=torch.float16) + t.to(dtype=torch.float16)) for s, t in zip(down_s, down_t)]
                mid_res = mid_s.to(dtype=torch.float16) + mid_t.to(dtype=torch.float16)

                # UNet å‰å‘ (åŒ…å« LoRA)
                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, 
                                 down_block_additional_residuals=down_res, 
                                 mid_block_additional_residual=mid_res).sample

                # æŸå¤±è®¡ç®— (ä»…ä¿ç•™ MSE å’Œ ç»“æ„å¼•å¯¼)
                loss_ddpm = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")
                
                loss_struct = torch.tensor(0.0).to(device)
                if rand_dropout >= 0.075: 
                    cond_resized = F.interpolate(cond_s, size=mid_s.shape[-2:], mode="bilinear")
                    loss_struct = F.l1_loss(mid_s.mean(dim=1, keepdim=True), cond_resized.mean(dim=1, keepdim=True))
                
                total_loss = loss_ddpm + args.lambda_struct * loss_struct
                
                accelerator.backward(total_loss)
                optimizer.step()
                optimizer.zero_grad()
            
            train_loss_epoch += total_loss.item()
            global_step += 1
            
            if global_step % args.checkpointing_steps == 0 and accelerator.is_main_process:
                ckpt_dir = Path(args.output_dir) / f"checkpoint-{global_step}"
                os.makedirs(ckpt_dir, exist_ok=True)
                
                accelerator.unwrap_model(controlnet_s).save_pretrained(ckpt_dir / "structure")
                accelerator.unwrap_model(controlnet_t).save_pretrained(ckpt_dir / "style")
                
                # [SAFE SAVE] åªä¿å­˜ LoRA æƒé‡ï¼Œä¸ä¿å­˜ 3GB çš„ UNet
                accelerator.unwrap_model(unet).save_pretrained(ckpt_dir / "unet_lora")
                print(f"ğŸ’¾ Checkpoint saved at step {global_step} (LoRA Safe Mode)")

            if step % 50 == 0 and accelerator.is_main_process:
                print(f"E{epoch} S{step} | Total: {total_loss:.4f} (DDPM: {loss_ddpm:.4f} | Struct: {loss_struct:.4f})")

        # === éªŒè¯é˜¶æ®µ ===
        if accelerator.is_main_process:
            print(f"ğŸ” Epoch {epoch}: æ­£åœ¨è¿›è¡ŒéªŒè¯é›†è¯„ä¼°...")
        
        controlnet_s.eval()
        controlnet_t.eval()
        unet.eval() # éªŒè¯æ—¶ LoRA ä¹Ÿä¼šè¢«å†»ç»“
        val_loss_total = 0.0
        val_steps = 0
        
        with torch.no_grad():
            for batch in val_dataloader:
                target_images = batch["pixel_values"].to(dtype=torch.float16)
                latents = vae.encode(target_images).latent_dist.sample() * vae.config.scaling_factor
                noise = torch.randn_like(latents)
                timesteps = torch.randint(0, 1000, (latents.shape[0],), device=latents.device).long()
                noisy_latents = scheduler.add_noise(latents, noise, timesteps)
                
                encoder_hidden_states = text_encoder(batch["input_ids"])[0]
                cond_image = batch["conditioning_pixel_values"].to(dtype=torch.float16)
                
                down_s, mid_s = controlnet_s(noisy_latents, timesteps, encoder_hidden_states, cond_image, return_dict=False)
                down_t, mid_t = controlnet_t(noisy_latents, timesteps, encoder_hidden_states, cond_image, return_dict=False)
                down_res = [(s.to(dtype=torch.float16) + t.to(dtype=torch.float16)) for s, t in zip(down_s, down_t)]
                mid_res = mid_s.to(dtype=torch.float16) + mid_t.to(dtype=torch.float16)
                
                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, 
                                  down_block_additional_residuals=down_res, 
                                  mid_block_additional_residual=mid_res).sample
                                  
                loss_ddpm = F.mse_loss(model_pred.float(), noise.float(), reduction="mean")
                val_loss_total += loss_ddpm.item()
                val_steps += 1
        
        avg_val_loss = val_loss_total / val_steps if val_steps > 0 else 0
        
        if accelerator.is_main_process:
            avg_train_loss = train_loss_epoch / len(train_dataloader)
            print(f"ğŸ“Š Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

        # --- éªŒè¯é‡‡æ · ---
        if accelerator.is_main_process:
            print(f"ğŸ¨ Epoch {epoch} ç»“æŸï¼Œä½¿ç”¨éªŒè¯é›†ç”Ÿæˆæ ·å›¾...")
            with torch.autocast(device.type, dtype=torch.float16):
                with torch.no_grad():
                    unwrapped_s = accelerator.unwrap_model(controlnet_s)
                    unwrapped_t = accelerator.unwrap_model(controlnet_t)
                    # æ­¤æ—¶ unwrapped_unet åŒ…å« LoRA å‚æ•°
                    unwrapped_unet = accelerator.unwrap_model(unet)
                    
                    # æ‰‹åŠ¨æ„å»º Pipelineï¼Œå¤ç”¨å†…å­˜ä¸­çš„ç»„ä»¶
                    val_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
                    pipe = StableDiffusionControlNetPipeline(
                        vae=vae,
                        text_encoder=text_encoder,
                        tokenizer=tokenizer,
                        unet=unwrapped_unet, # å¸¦æœ‰ LoRA çš„ UNet
                        controlnet=[unwrapped_s, unwrapped_t],
                        scheduler=val_scheduler,
                        safety_checker=None,
                        feature_extractor=None
                    ).to(device)
                    
                    test_prompt = batch["texts"][0] if len(batch["texts"]) > 0 else "ä¸­å›½æ°´å¢¨å±±æ°´ç”»"
                    test_cond = cond_image[0:1].to(device=device, dtype=torch.float16)
                    
                    # ä¿å­˜
                    layout_img_pil = transforms.ToPILImage()(test_cond.squeeze(0).cpu())
                    os.makedirs(args.output_dir, exist_ok=True)
                    layout_img_pil.save(Path(args.output_dir) / f"layout_epoch_{epoch}_val.png")

                    sample_out = pipe(
                        prompt=test_prompt, 
                        image=[test_cond, test_cond], 
                        num_inference_steps=20,
                        guidance_scale=7.5
                    ).images[0]
                    
                    sample_out.save(Path(args.output_dir) / f"sample_epoch_{epoch}_val.png")
                    print(f"âœ… éªŒè¯å›¾å·²ä¿å­˜")
                    
                    del pipe, val_scheduler
                    torch.cuda.empty_cache()

    if accelerator.is_main_process:
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        save_path_s = Path(args.output_dir) / "controlnet_structure"
        save_path_t = Path(args.output_dir) / "controlnet_style"
        os.makedirs(save_path_s, exist_ok=True)
        os.makedirs(save_path_t, exist_ok=True)
        
        accelerator.unwrap_model(controlnet_s).save_pretrained(save_path_s)
        accelerator.unwrap_model(controlnet_t).save_pretrained(save_path_t)
        # ä»…ä¿å­˜ LoRA æƒé‡ (å®‰å…¨ã€è½»é‡)
        accelerator.unwrap_model(unet).save_pretrained(Path(args.output_dir) / "unet_lora")
        print(f"âœ… è®­ç»ƒåœ†æ»¡å®Œæˆï¼ŒLoRA ä¸ ControlNets å·²ä¿å­˜è‡³: {args.output_dir}")

if __name__ == "__main__":
    main()